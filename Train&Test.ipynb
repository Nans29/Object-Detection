{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1788c5a-a4fc-446c-949e-ea6e6425a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import itertools\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense , Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17aa4469-1bf9-4692-97a5-e6fdc3770e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "img_height = 128\n",
    "img_width = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12189e42-2570-4bac-a16b-bc0fb6c2c518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1288 files belonging to 8 classes.\n",
      "Using 1031 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    " 'data2/train',\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    "  color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "869cbe84-9273-4d47-86af-4d14576e8051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1288 files belonging to 8 classes.\n",
      "Using 257 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  'data2/train',\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    "  color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4488c6d8-9c61-4307-a1b9-88dca4784c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Book', 'Comb', 'Cup', 'Glasses', 'Mouse', 'Pen', 'Remote', 'Scissors']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a131197e-d8cb-4712-89eb-41718deb55c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 128, 128, 1)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36c8fb68-c2bb-46e2-a0ba-e04fd0bdd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in val_ds:\n",
    "    images_val = images.numpy()\n",
    "    labels_val = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "884e3673-b49a-4f24-93f4-8fd86d3ef000",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b74bb306-6fba-42df-b260-b8da92d77962",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53d41ff5-5f55-4b40-b095-5507376325ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "41020435-e511-47a9-b40f-b0ba6e19583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 1)),\n",
    "  layers.Conv2D(100, 5, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(100, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0536abdd-3c03-49e3-a5fa-04d8d4909f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0eb36926-4425-4ace-91f3-0068a8b3c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_5 (Rescaling)     (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 128, 128, 100)     2600      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 64, 64, 100)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 64, 64, 100)       90100     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 32, 32, 100)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32, 32, 100)       0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 102400)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               13107328  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,201,060\n",
      "Trainable params: 13,201,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cabb8c0d-8e85-4579-86c4-7af907e233b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad69357c-7843-4ae7-aa8e-9ebd245ae59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "    \n",
    "    # Save the plot to a PNG in memory.\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside the notebook.\n",
    "    plt.close(figure)\n",
    "    \n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "850e4f04-b9ea-4c82-8333-a883eb2fa9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a file writer variable for logging purposes\n",
    "file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')\n",
    "\n",
    "def log_confusion_matrix(epoch, logs):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "    test_pred_raw = model.predict(images_val)\n",
    "    test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "    # Calculate the confusion matrix.\n",
    "    cm = sklearn.metrics.confusion_matrix(labels_val, test_pred)\n",
    "    \n",
    "    # Log the confusion matrix as an image summary.\n",
    "    figure = plot_confusion_matrix(cm, class_names=['Book', 'Comb', 'Cup', 'Glasses', 'Mouse', 'Pen', 'Remote', 'Scissors'])\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d468b138-7cc1-47f8-9f60-0b56bae5fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the callbacks\n",
    "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "548aa0d4-ea93-40ce-8b02-296ab54fbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'auto',\n",
    "    min_delta = 0,\n",
    "    patience = 2,\n",
    "    verbose = 0, \n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "26d25884-0f35-49de-b7a1-b6a4b943cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "65/65 [==============================] - 62s 926ms/step - loss: 1.3764 - accuracy: 0.5664 - val_loss: 0.2608 - val_accuracy: 0.9416\n",
      "Epoch 2/20\n",
      "65/65 [==============================] - 60s 922ms/step - loss: 0.1322 - accuracy: 0.9641 - val_loss: 0.2029 - val_accuracy: 0.9728\n",
      "Epoch 3/20\n",
      "65/65 [==============================] - 67s 1s/step - loss: 0.0451 - accuracy: 0.9855 - val_loss: 0.1026 - val_accuracy: 0.9767\n",
      "Epoch 4/20\n",
      "65/65 [==============================] - 55s 847ms/step - loss: 0.0465 - accuracy: 0.9952 - val_loss: 0.1067 - val_accuracy: 0.9844\n",
      "Epoch 5/20\n",
      "65/65 [==============================] - 63s 971ms/step - loss: 0.0232 - accuracy: 0.9913 - val_loss: 0.0922 - val_accuracy: 0.9883\n",
      "Epoch 6/20\n",
      "65/65 [==============================] - 59s 902ms/step - loss: 0.0245 - accuracy: 0.9942 - val_loss: 0.1219 - val_accuracy: 0.9883\n",
      "Epoch 7/20\n",
      "65/65 [==============================] - 59s 912ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x207d529ca90>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=20\n",
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  callbacks = [tensorboard_callback, cm_callback,early_stopping],\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8eda4c4b-91a1-4fa9-9baf-0d8c0bb09c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Weights saved\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model-bw.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print('Model Saved')\n",
    "model.save_weights('model-bw.h5')\n",
    "print('Weights saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "93eeba68-863f-4316-8754-e1c750103765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 328 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  'data2/test',\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    "  color_mode='grayscale')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e1343b6-d258-4a41-90ba-6bbbd8e06f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Book', 'Comb', 'Cup', 'Glasses', 'Mouse', 'Pen', 'Remote', 'Scissors']\n"
     ]
    }
   ],
   "source": [
    "class_names = test_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "75cfd233-97d8-4031-8c16-12f234ffca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 6s 183ms/step - loss: 2.3188 - accuracy: 0.7012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3187615871429443, 0.7012194991111755]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b9c66950-6986-49a4-8846-8024aee8cb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to Mouse with a 57.95 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = tf.keras.utils.load_img(\n",
    "    \"./data/test/Comb/4.jpg\", target_size=(img_height, img_width,),grayscale=True\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b56d7-83e4-44b1-be6f-00acaa12c9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a7453b-e746-4884-afc9-7a0d65678629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
